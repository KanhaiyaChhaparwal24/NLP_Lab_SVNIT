{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc_Pv1x1XpVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60118a3-e94f-4621-d531-f26eec8741f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी। 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी। उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था। उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है। ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Stream Hindi dataset from Hugging Face without full download\n",
        "hindi_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Example: read first 5 lines\n",
        "for i, item in enumerate(hindi_dataset):\n",
        "    print(item[\"text\"])\n",
        "    if i == 4:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "def is_devanagari_char(ch):\n",
        "    # Check if char belongs to Devanagari block by Unicode name\n",
        "    try:\n",
        "        return 'DEVANAGARI' in unicodedata.name(ch)\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def is_digit(ch):\n",
        "    return unicodedata.category(ch) == 'Nd'  # Decimal number\n",
        "\n",
        "def is_punctuation(ch):\n",
        "    return unicodedata.category(ch).startswith('P')  # Any punctuation\n",
        "\n",
        "def is_latin_char(ch):\n",
        "    try:\n",
        "        return 'LATIN' in unicodedata.name(ch)\n",
        "    except ValueError:\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "Ep6lVdIkdH-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_word(word):\n",
        "    tokens = []\n",
        "    current_token = \"\"\n",
        "    current_type = None  # 'devanagari', 'digit', 'latin', 'punct', 'other'\n",
        "\n",
        "    for ch in word:\n",
        "        if is_devanagari_char(ch):\n",
        "            ch_type = 'devanagari'\n",
        "        elif is_digit(ch):\n",
        "            ch_type = 'digit'\n",
        "        elif is_latin_char(ch):\n",
        "            ch_type = 'latin'\n",
        "        elif is_punctuation(ch):\n",
        "            ch_type = 'punct'\n",
        "        else:\n",
        "            ch_type = 'other'\n",
        "\n",
        "        if current_type == ch_type or current_type is None:\n",
        "            current_token += ch\n",
        "            current_type = ch_type\n",
        "        else:\n",
        "            tokens.append(current_token)\n",
        "            current_token = ch\n",
        "            current_type = ch_type\n",
        "\n",
        "    if current_token:\n",
        "        tokens.append(current_token)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    sentences = []\n",
        "    sentence = \"\"\n",
        "    for ch in text:\n",
        "        sentence += ch\n",
        "        if ch in ['.', '!', '?', '।']:\n",
        "            sentences.append(sentence.strip())\n",
        "            sentence = \"\"\n",
        "    if sentence.strip():\n",
        "        sentences.append(sentence.strip())\n",
        "    return sentences\n",
        "\n",
        "def word_tokenize(sentence):\n",
        "    words = sentence.split()\n",
        "    all_tokens = []\n",
        "    for word in words:\n",
        "        if word.startswith((\"http://\", \"https://\", \"www.\")) or \"@\" in word:\n",
        "            all_tokens.append(word)\n",
        "        else:\n",
        "            all_tokens.extend(tokenize_word(word))\n",
        "    return all_tokens\n"
      ],
      "metadata": {
        "id": "PqXf6XaMelB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_PARAGRAPHS = 5\n",
        "count = 0\n",
        "\n",
        "for item in hindi_dataset:\n",
        "    paragraph = item[\"text\"].strip()\n",
        "    if not paragraph:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nParagraph {count+1}:\\n{paragraph}\\n\")\n",
        "\n",
        "    sentences = sentence_tokenize(paragraph)\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        print(f\"Sentence: {sentence}\")\n",
        "        print(f\"Tokens: {tokens}\\n\")\n",
        "\n",
        "    count += 1\n",
        "    if count >= MAX_PARAGRAPHS:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7JBRZX6icnh",
        "outputId": "4316c00e-38a6-4353-d112-f34b686baede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Paragraph 1:\n",
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Sentence: लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "Tokens: ['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम']\n",
            "\n",
            "\n",
            "Paragraph 2:\n",
            "इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी। 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी। उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था। उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है। ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "Sentence: इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "Tokens: ['इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री', 'देवीलाल', 'ने', 'अपने', 'पुत्र', 'ओमप्रकाश', 'चौटाला', 'को', 'अपना', 'राजनीतिक', 'उत्तराधिकारी', 'घोषित', 'किया', 'था।']\n",
            "\n",
            "Sentence: हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "Tokens: ['हालांकि', 'तब', 'पार्टी', 'पर', 'देवीलाल', 'की', 'मजबूत', 'पकड़', 'के', 'चलते', 'पार्टी', 'टूटने', 'से', 'बच', 'गई', 'थी।']\n",
            "\n",
            "Sentence: 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी।\n",
            "Tokens: ['1989', 'में', 'देवीलाल', 'केन्द्र', 'की', 'राजनीति', 'में', 'सक्रिय', 'हो', 'गए', 'थे', 'और', 'उनके', 'उपप्रधानमंत्री', 'बनने', 'के', 'पश्चात्', 'उनके', 'तीन', 'बेटों', 'जगदीश', 'सिंह', ',', 'रणजीत', 'सिंह', 'और', 'ओमप्रकाश', 'चौटाला', 'में', 'से', 'रणजीत', 'और', 'ओमप्रकाश', 'के', 'बीच', 'हरियाणा', 'में', 'उनकी', 'राजनीतिक', 'विरासत', 'को', 'लेकर', 'जंग', 'शुरू', 'हो', 'गई', 'थी।']\n",
            "\n",
            "Sentence: उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था।\n",
            "Tokens: ['उन', 'परिस्थितियों', 'में', 'देवीलाल', 'ने', 'कड़ा', 'निर्णय', 'लेते', 'हुए', 'पार्टी', 'की', 'बागडोर', 'ओमप्रकाश', 'चौटाला', 'के', 'हवाले', 'कर', 'दी', 'थी', ',', 'जिसके', 'बाद', 'रणजीत', 'की', 'बगावत', 'का', 'असर', 'पार्टी', ',', 'संगठन', 'और', 'उनकी', 'सरकार', 'पर', 'भी', 'पड़ा', 'था।']\n",
            "\n",
            "Sentence: उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ.\n",
            "Tokens: ['उस', 'समय', 'रणजीत', 'की', 'नाराजगी', 'के', 'चलते', 'उनके', 'समर्थन', 'में', 'कई', 'कैबिनेट', 'मंत्रियों', 'ने', 'इस्तीफे', 'दे', 'दिए', 'थे', 'किन्तु', 'तब', 'पार्टी', 'सुप्रीमो', 'चौ', '.']\n",
            "\n",
            "Sentence: देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है।\n",
            "Tokens: ['देवीलाल', 'की', 'हरियाणा', 'की', 'जनता', 'पर', 'इतनी', 'मजबूत', 'पकड़', 'थी', 'कि', 'ओमप्रकाश', 'चौटाला', 'को', 'उत्तराधिकारी', 'बनाने', 'के', 'उनके', 'फैसले', 'का', 'जनता', 'के', 'बीच', 'कोई', 'खास', 'विरोध', 'नहीं', 'हुआ', 'था', 'लेकिन', 'आज', 'स्थिति', 'बिल्कुल', 'विपरीत', 'है।']\n",
            "\n",
            "Sentence: ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "Tokens: ['ओमप्रकाश', 'चौटाला', 'पिछले', 'काफी', 'समय', 'से', 'जेल', 'में', 'हैं', 'और', 'जेल', 'में', 'रहते', 'पार्टी', 'के', 'साथ', '-', 'साथ', 'परिवार', 'पर', 'भी', 'उनकी', 'पकड़', 'काफी', 'ढ़ीली', 'हो', 'गई', 'है', ',', 'इसी', 'कारण', 'उनमें', 'अब', 'देवीलाल', 'जैसा', 'वो', 'सामर्थ्य', 'नजर', 'नहीं', 'आता', 'कि', 'वे', 'अपने', 'फैसलों', 'को', 'बगैर', 'किसी', 'प्रतिरोध', 'के', 'लागू', 'करा', 'सकें।']\n",
            "\n",
            "\n",
            "Paragraph 3:\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Sentence: जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "Tokens: ['जहां', 'आई', 'थी', 'तबाही', 'उस', 'घाटी', 'क्षेत्र', 'में', 'खतरा', 'ज्यादा']\n",
            "\n",
            "\n",
            "Paragraph 4:\n",
            "इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई। भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भेजा है।\n",
            "\n",
            "Sentence: इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई।\n",
            "Tokens: ['इसके', 'बाद', 'केंद्र', 'की', 'ओर', 'से', 'प्रदेश', 'सरकार', 'को', 'पीएमजीएसवाई', 'में', '200', 'करोड़', 'रुपये', 'की', 'राशि', 'उपलब्ध', 'करा', 'दी', 'गई।']\n",
            "\n",
            "Sentence: भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भेजा है।\n",
            "Tokens: ['भाजपा', 'के', 'मीडिया', 'प्रभारी', 'दिवाकर', 'सिंह', 'ने', 'शनिवार', 'को', 'बताया', 'कि', 'केंद्र', 'ने', 'प्रदेश', 'सरकार', 'को', '200', 'करोड़', 'रुपये', 'भेजा', 'है।']\n",
            "\n",
            "\n",
            "Paragraph 5:\n",
            "यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा, \"हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था. हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा. इसमें सिर्फ बेसिक्स पर अडिग रहना होगा और साथ ही शांत चित्त बने रहना होगा. हमारी तैयारी हमारे हाथों में हैं और हमने अपना शत प्रतिशत दिया है. हमें मैच के दिन अच्छी क्रिकेट खेलने की उम्मीद है.\"\n",
            "\n",
            "Sentence: यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा, \"हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था.\n",
            "Tokens: ['यह', 'पूछने', 'पर', 'कि', 'इस', 'बड़े', 'मैच', 'से', 'पहले', 'उनकी', 'नींद', 'गायब', 'हुई', 'तो', 'बाबर', 'ने', 'कहा', ',', '\"', 'हम', 'काफी', 'टूर्नामेंट', 'खेल', 'चुके', 'हैं', ',', 'हमने', 'चैम्पियंस', 'ट्राफी', 'में', 'भी', 'अच्छा', 'किया', 'था', '.']\n",
            "\n",
            "Sentence: हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा.\n",
            "Tokens: ['हम', 'इसे', 'जितना', 'सरल', 'रखेंगे', ',', 'उतना', 'ही', 'बेहतर', 'होगा', '.']\n",
            "\n",
            "Sentence: इसमें सिर्फ बेसिक्स पर अडिग रहना होगा और साथ ही शांत चित्त बने रहना होगा.\n",
            "Tokens: ['इसमें', 'सिर्फ', 'बेसिक्स', 'पर', 'अडिग', 'रहना', 'होगा', 'और', 'साथ', 'ही', 'शांत', 'चित्त', 'बने', 'रहना', 'होगा', '.']\n",
            "\n",
            "Sentence: हमारी तैयारी हमारे हाथों में हैं और हमने अपना शत प्रतिशत दिया है.\n",
            "Tokens: ['हमारी', 'तैयारी', 'हमारे', 'हाथों', 'में', 'हैं', 'और', 'हमने', 'अपना', 'शत', 'प्रतिशत', 'दिया', 'है', '.']\n",
            "\n",
            "Sentence: हमें मैच के दिन अच्छी क्रिकेट खेलने की उम्मीद है.\n",
            "Tokens: ['हमें', 'मैच', 'के', 'दिन', 'अच्छी', 'क्रिकेट', 'खेलने', 'की', 'उम्मीद', 'है', '.']\n",
            "\n",
            "Sentence: \"\n",
            "Tokens: ['\"']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RE"
      ],
      "metadata": {
        "id": "rMFDzDDUo0GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load 5 lines from the Hindi dataset\n",
        "hindi_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# Unicode Block: Devanagari range for Hindi (U+0900 to U+097F)\n",
        "def is_hindi_char(char):\n",
        "    return '\\u0900' <= char <= '\\u097F'\n",
        "\n",
        "# Sentence tokenizer (split on । or sentence-ending punctuation)\n",
        "def sentence_tokenize(text):\n",
        "    return re.split(r'(?<=[।!?])\\s+', text.strip())\n",
        "\n",
        "# Word tokenizer using regex to handle URLs, emails, numbers, punctuations\n",
        "def word_tokenize(sentence):\n",
        "    pattern = r'''(?x)               # verbose mode\n",
        "        (https?://\\S+|www\\.\\S+)      # URLs\n",
        "      | (\\w+@\\w+\\.\\w+)               # emails\n",
        "      | (\\d+\\.\\d+|\\d+)               # numbers (integers, decimals)\n",
        "      | ([\\u0900-\\u097F]+)           # Hindi words (Devanagari)\n",
        "      | ([^\\s\\w])                    # punctuation\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence)\n",
        "    # Each match is a tuple with only one non-empty value\n",
        "    return [token for group in tokens for token in group if token]\n",
        "\n",
        "# Stream and process first 5 lines\n",
        "for i, item in enumerate(hindi_dataset):\n",
        "    paragraph = item[\"text\"].strip()\n",
        "    if not paragraph:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n Paragraph :\")\n",
        "    print(paragraph)\n",
        "\n",
        "    # Unicode block check (optional)\n",
        "    devnagari_chars = [char for char in paragraph if is_hindi_char(char)]\n",
        "    if not devnagari_chars:\n",
        "        print(\"  Not detected as Hindi (Devanagari)\")\n",
        "        continue\n",
        "\n",
        "    # Sentence tokenization\n",
        "    sentences = sentence_tokenize(paragraph)\n",
        "    for sent_num, sentence in enumerate(sentences, 1):\n",
        "        print(f\"\\n  Sentence {sent_num}: {sentence}\")\n",
        "        tokens = word_tokenize(sentence)\n",
        "        print(f\"      Tokens: {tokens}\")\n",
        "\n",
        "    if i == 4:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNeqbQdtjeXX",
        "outputId": "3e299403-c392-47e1-840e-e108066fd5d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Paragraph :\n",
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "  Sentence 1: लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "      Tokens: ['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम']\n",
            "\n",
            " Paragraph :\n",
            "इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी। 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी। उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था। उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है। ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "\n",
            "  Sentence 1: इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "      Tokens: ['इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री', 'देवीलाल', 'ने', 'अपने', 'पुत्र', 'ओमप्रकाश', 'चौटाला', 'को', 'अपना', 'राजनीतिक', 'उत्तराधिकारी', 'घोषित', 'किया', 'था।']\n",
            "\n",
            "  Sentence 2: हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "      Tokens: ['हालांकि', 'तब', 'पार्टी', 'पर', 'देवीलाल', 'की', 'मजबूत', 'पकड़', 'के', 'चलते', 'पार्टी', 'टूटने', 'से', 'बच', 'गई', 'थी।']\n",
            "\n",
            "  Sentence 3: 1989 में देवीलाल केन्द्र की राजनीति में सक्रिय हो गए थे और उनके उपप्रधानमंत्री बनने के पश्चात् उनके तीन बेटों जगदीश सिंह, रणजीत सिंह और ओमप्रकाश चौटाला में से रणजीत और ओमप्रकाश के बीच हरियाणा में उनकी राजनीतिक विरासत को लेकर जंग शुरू हो गई थी।\n",
            "      Tokens: ['1989', 'में', 'देवीलाल', 'केन्द्र', 'की', 'राजनीति', 'में', 'सक्रिय', 'हो', 'गए', 'थे', 'और', 'उनके', 'उपप्रधानमंत्री', 'बनने', 'के', 'पश्चात्', 'उनके', 'तीन', 'बेटों', 'जगदीश', 'सिंह', ',', 'रणजीत', 'सिंह', 'और', 'ओमप्रकाश', 'चौटाला', 'में', 'से', 'रणजीत', 'और', 'ओमप्रकाश', 'के', 'बीच', 'हरियाणा', 'में', 'उनकी', 'राजनीतिक', 'विरासत', 'को', 'लेकर', 'जंग', 'शुरू', 'हो', 'गई', 'थी।']\n",
            "\n",
            "  Sentence 4: उन परिस्थितियों में देवीलाल ने कड़ा निर्णय लेते हुए पार्टी की बागडोर ओमप्रकाश चौटाला के हवाले कर दी थी, जिसके बाद रणजीत की बगावत का असर पार्टी, संगठन और उनकी सरकार पर भी पड़ा था।\n",
            "      Tokens: ['उन', 'परिस्थितियों', 'में', 'देवीलाल', 'ने', 'कड़ा', 'निर्णय', 'लेते', 'हुए', 'पार्टी', 'की', 'बागडोर', 'ओमप्रकाश', 'चौटाला', 'के', 'हवाले', 'कर', 'दी', 'थी', ',', 'जिसके', 'बाद', 'रणजीत', 'की', 'बगावत', 'का', 'असर', 'पार्टी', ',', 'संगठन', 'और', 'उनकी', 'सरकार', 'पर', 'भी', 'पड़ा', 'था।']\n",
            "\n",
            "  Sentence 5: उस समय रणजीत की नाराजगी के चलते उनके समर्थन में कई कैबिनेट मंत्रियों ने इस्तीफे दे दिए थे किन्तु तब पार्टी सुप्रीमो चौ. देवीलाल की हरियाणा की जनता पर इतनी मजबूत पकड़ थी कि ओमप्रकाश चौटाला को उत्तराधिकारी बनाने के उनके फैसले का जनता के बीच कोई खास विरोध नहीं हुआ था लेकिन आज स्थिति बिल्कुल विपरीत है।\n",
            "      Tokens: ['उस', 'समय', 'रणजीत', 'की', 'नाराजगी', 'के', 'चलते', 'उनके', 'समर्थन', 'में', 'कई', 'कैबिनेट', 'मंत्रियों', 'ने', 'इस्तीफे', 'दे', 'दिए', 'थे', 'किन्तु', 'तब', 'पार्टी', 'सुप्रीमो', 'चौ', '.', 'देवीलाल', 'की', 'हरियाणा', 'की', 'जनता', 'पर', 'इतनी', 'मजबूत', 'पकड़', 'थी', 'कि', 'ओमप्रकाश', 'चौटाला', 'को', 'उत्तराधिकारी', 'बनाने', 'के', 'उनके', 'फैसले', 'का', 'जनता', 'के', 'बीच', 'कोई', 'खास', 'विरोध', 'नहीं', 'हुआ', 'था', 'लेकिन', 'आज', 'स्थिति', 'बिल्कुल', 'विपरीत', 'है।']\n",
            "\n",
            "  Sentence 6: ओमप्रकाश चौटाला पिछले काफी समय से जेल में हैं और जेल में रहते पार्टी के साथ-साथ परिवार पर भी उनकी पकड़ काफी ढ़ीली हो गई है, इसी कारण उनमें अब देवीलाल जैसा वो सामर्थ्य नजर नहीं आता कि वे अपने फैसलों को बगैर किसी प्रतिरोध के लागू करा सकें।\n",
            "      Tokens: ['ओमप्रकाश', 'चौटाला', 'पिछले', 'काफी', 'समय', 'से', 'जेल', 'में', 'हैं', 'और', 'जेल', 'में', 'रहते', 'पार्टी', 'के', 'साथ', '-', 'साथ', 'परिवार', 'पर', 'भी', 'उनकी', 'पकड़', 'काफी', 'ढ़ीली', 'हो', 'गई', 'है', ',', 'इसी', 'कारण', 'उनमें', 'अब', 'देवीलाल', 'जैसा', 'वो', 'सामर्थ्य', 'नजर', 'नहीं', 'आता', 'कि', 'वे', 'अपने', 'फैसलों', 'को', 'बगैर', 'किसी', 'प्रतिरोध', 'के', 'लागू', 'करा', 'सकें।']\n",
            "\n",
            " Paragraph :\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "  Sentence 1: जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "      Tokens: ['जहां', 'आई', 'थी', 'तबाही', 'उस', 'घाटी', 'क्षेत्र', 'में', 'खतरा', 'ज्यादा']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "import unicodedata\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "hindi_dataset = load_dataset(\n",
        "    \"text\",\n",
        "    data_files=\"https://huggingface.co/datasets/ai4bharat/IndicCorpV2/resolve/main/data/hi-1.txt\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "def sentence_split(paragraph):\n",
        "    sentence_endings = ['.', '?', '!', '।']\n",
        "    sentences = []\n",
        "    current = ''\n",
        "    for char in paragraph:\n",
        "        current += char\n",
        "        if char in sentence_endings:\n",
        "            if current.strip():\n",
        "                sentences.append(current.strip())\n",
        "                current = ''\n",
        "    if current.strip():\n",
        "        sentences.append(current.strip())\n",
        "    return sentences\n",
        "\n",
        "def word_tokenize(sentence):\n",
        "    pattern = r'''\n",
        "        (https?://[^\\s]+) |                     # Group 1: URLs\n",
        "        (www\\.[^\\s]+) |                         # Group 2: www URLs\n",
        "        (\\w+@\\w+\\.\\w+) |                        # Group 3: Emails\n",
        "        (\\d{1,2}/\\d{1,2}/\\d{2,4}) |             # Group 4: Dates\n",
        "        (\\d+\\.\\d+) |                            # Group 5: Decimal numbers\n",
        "        ([\\u0900-\\u097F]+) |                    # Group 6: Hindi (Devanagari) words\n",
        "        ([a-zA-Z0-9_-]+) |                      # Group 7: Latin words/digits\n",
        "        ([^\\s])                                 # Group 8: Other single chars\n",
        "    '''\n",
        "    tokens = re.findall(pattern, sentence, re.VERBOSE)\n",
        "    flat_tokens = [token for group in tokens for token in group if token]\n",
        "    return flat_tokens\n",
        "\n",
        "class CorpusStatistics:\n",
        "    def __init__(self):\n",
        "        self.total_sentences = 0\n",
        "        self.total_words = 0\n",
        "        self.total_characters = 0\n",
        "        self.sentence_lengths = []\n",
        "        self.word_lengths = []\n",
        "        self.vocabulary = Counter()\n",
        "        self.processed_documents = 0\n",
        "        self.tokenized_data = []\n",
        "\n",
        "    def process_document(self, text: str, doc_id: int) -> Dict[str, Any]:\n",
        "        \"\"\"Process a single document and update statistics\"\"\"\n",
        "        if not text or not text.strip():\n",
        "            return None\n",
        "\n",
        "        text = text.strip()\n",
        "        sentences = sentence_split(text)\n",
        "\n",
        "        processed_sentences = []\n",
        "        doc_word_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if not sentence.strip():\n",
        "                continue\n",
        "\n",
        "            words = word_tokenize(sentence)\n",
        "\n",
        "            if words:\n",
        "                processed_sentences.append({\n",
        "                    'text': sentence,\n",
        "                    'tokens': words,\n",
        "                    'word_count': len(words)\n",
        "                })\n",
        "\n",
        "                self.total_sentences += 1\n",
        "                self.total_words += len(words)\n",
        "                self.total_characters += len(sentence)\n",
        "                self.sentence_lengths.append(len(words))\n",
        "\n",
        "                self.vocabulary.update(words)\n",
        "\n",
        "                for word in words:\n",
        "                    self.word_lengths.append(len(word))\n",
        "\n",
        "                doc_word_count += len(words)\n",
        "\n",
        "        if processed_sentences:\n",
        "            self.processed_documents += 1\n",
        "\n",
        "            document_data = {\n",
        "                'document_id': doc_id,\n",
        "                'original_text': text,\n",
        "                'sentences': processed_sentences,\n",
        "                'document_stats': {\n",
        "                    'sentence_count': len(processed_sentences),\n",
        "                    'word_count': doc_word_count,\n",
        "                    'character_count': len(text)\n",
        "                }\n",
        "            }\n",
        "\n",
        "            return document_data\n",
        "\n",
        "        return None\n",
        "\n",
        "    def compute_final_statistics(self) -> Dict[str, float]:\n",
        "        \"\"\"Compute all required corpus statistics\"\"\"\n",
        "        if self.total_sentences == 0:\n",
        "            return {}\n",
        "\n",
        "        avg_sentence_length = (sum(self.sentence_lengths) / len(self.sentence_lengths)) if self.sentence_lengths else 0\n",
        "        avg_word_length = (sum(self.word_lengths) / len(self.word_lengths)) if self.word_lengths else 0\n",
        "\n",
        "        unique_tokens = len(self.vocabulary)\n",
        "        total_tokens = self.total_words\n",
        "        ttr = unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'total_sentences': self.total_sentences,\n",
        "            'total_words': self.total_words,\n",
        "            'total_characters': self.total_characters,\n",
        "            'average_sentence_length': round(avg_sentence_length, 2),\n",
        "            'average_word_length': round(avg_word_length, 2),\n",
        "            'type_token_ratio': round(ttr, 4),\n",
        "            'vocabulary_size': unique_tokens,\n",
        "            'processed_documents': self.processed_documents\n",
        "        }\n",
        "\n",
        "    def save_data_and_statistics(self, output_dir: str = \"hindi_corpus_output\"):\n",
        "        \"\"\"Save tokenized data and statistics to files - optimized for large datasets\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # For very large datasets, save in chunks to manage memory\n",
        "        chunk_size = 10000\n",
        "        if len(self.tokenized_data) > chunk_size:\n",
        "            print(f\"Saving large dataset in chunks of {chunk_size} documents...\")\n",
        "\n",
        "            for i in range(0, len(self.tokenized_data), chunk_size):\n",
        "                chunk = self.tokenized_data[i:i+chunk_size]\n",
        "                chunk_file = os.path.join(output_dir, f\"tokenized_data_chunk_{i//chunk_size + 1:03d}.json\")\n",
        "                with open(chunk_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(chunk, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # Save complete data in pickle format (more memory efficient)\n",
        "            pickle_file = os.path.join(output_dir, \"tokenized_data_complete.pkl\")\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(self.tokenized_data, f)\n",
        "\n",
        "        else:\n",
        "            # Save tokenized data in JSON format\n",
        "            json_file = os.path.join(output_dir, \"tokenized_data.json\")\n",
        "            with open(json_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.tokenized_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # Save tokenized data in pickle format (faster loading)\n",
        "            pickle_file = os.path.join(output_dir, \"tokenized_data.pkl\")\n",
        "            with open(pickle_file, 'wb') as f:\n",
        "                pickle.dump(self.tokenized_data, f)\n",
        "\n",
        "        # Compute and save statistics\n",
        "        stats = self.compute_final_statistics()\n",
        "\n",
        "        # Save statistics as JSON\n",
        "        stats_file = os.path.join(output_dir, \"corpus_statistics.json\")\n",
        "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save top vocabulary (to manage file size for large corpora)\n",
        "        vocab_file = os.path.join(output_dir, \"vocabulary_top10000.json\")\n",
        "        vocab_dict = dict(self.vocabulary.most_common(10000))  # Top 10K words\n",
        "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(vocab_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # Save complete vocabulary in pickle format\n",
        "        full_vocab_file = os.path.join(output_dir, \"vocabulary_complete.pkl\")\n",
        "        with open(full_vocab_file, 'wb') as f:\n",
        "            pickle.dump(dict(self.vocabulary), f)\n",
        "\n",
        "        report_file = os.path.join(output_dir, \"statistics_report.txt\")\n",
        "        with open(report_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"Dataset: ai4bharat/IndicCorpV2 (Hindi)\\n\")\n",
        "            f.write(\"File: hi-1.txt\\n\")\n",
        "            f.write(f\"Processing Scale: {stats['processed_documents']:,} documents\\n\\n\")\n",
        "\n",
        "            f.write(f\"i.   Total number of sentences: {stats['total_sentences']:,}\\n\")\n",
        "            f.write(f\"ii.  Total number of words: {stats['total_words']:,}\\n\")\n",
        "            f.write(f\"iii. Total number of characters: {stats['total_characters']:,}\\n\")\n",
        "            f.write(f\"iv.  Average Sentence Length: {stats['average_sentence_length']} words per sentence\\n\")\n",
        "            f.write(f\"v.   Average word length: {stats['average_word_length']} characters per word\\n\")\n",
        "            f.write(f\"vi.  Type/Token Ratio (TTR): {stats['type_token_ratio']}\\n\\n\")\n",
        "\n",
        "            f.write(\"EXTENDED STATISTICS:\\n\")\n",
        "            f.write(\"-\" * 20 + \"\\n\")\n",
        "            f.write(f\"Vocabulary size (unique tokens): {stats['vocabulary_size']:,}\\n\")\n",
        "            f.write(f\"Processed documents: {stats['processed_documents']:,}\\n\")\n",
        "            f.write(f\"Longest sentence: {max(self.sentence_lengths) if self.sentence_lengths else 0} words\\n\")\n",
        "            f.write(f\"Shortest sentence: {min(self.sentence_lengths) if self.sentence_lengths else 0} words\\n\")\n",
        "            f.write(f\"Longest word: {max(self.word_lengths) if self.word_lengths else 0} characters\\n\\n\")\n",
        "\n",
        "            f.write(\"TOP 50 MOST FREQUENT TOKENS:\\n\")\n",
        "            f.write(\"-\" * 30 + \"\\n\")\n",
        "            for i, (word, freq) in enumerate(self.vocabulary.most_common(50), 1):\n",
        "                f.write(f\"{i:2d}. {word}: {freq:,}\\n\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "def main():\n",
        "    # Initialize corpus statistics\n",
        "    corpus_stats = CorpusStatistics()\n",
        "\n",
        "    print(\"\\nProcessing Hindi dataset...\\n\")\n",
        "\n",
        "    # Process first few examples for demonstration (like your original code)\n",
        "    print(\"SAMPLE PROCESSING (First 5 examples):\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    sample_count = 0\n",
        "    for i, example in enumerate(hindi_dataset):\n",
        "        # Skip if 'text' is missing or just whitespace\n",
        "        if 'text' not in example or not example['text'].strip():\n",
        "            continue\n",
        "\n",
        "        text = example['text'].strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        # Show sample processing for first 5 examples\n",
        "        if sample_count < 5:\n",
        "            print(f\"\\n--- Example {sample_count + 1} ---\")\n",
        "            print(\"Original Paragraph:\")\n",
        "            print(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "\n",
        "            sentences = sentence_split(text)\n",
        "\n",
        "            print(f\"\\nTokenized Sentences ({len(sentences)} total):\")\n",
        "            for j, sent in enumerate(sentences[:2]):  # Show first 2 sentences\n",
        "                print(f\"{j+1}. {sent}\")\n",
        "            if len(sentences) > 2:\n",
        "                print(f\"... and {len(sentences) - 2} more sentences\")\n",
        "\n",
        "            print(\"\\nTokenized Words (first sentence):\")\n",
        "            if sentences:\n",
        "                tokens = word_tokenize(sentences[0])\n",
        "                print(tokens[:15])  # Show first 15 tokens\n",
        "                if len(tokens) > 15:\n",
        "                    print(f\"... and {len(tokens) - 15} more tokens\")\n",
        "\n",
        "            sample_count += 1\n",
        "\n",
        "        # Process document for statistics\n",
        "        processed_doc = corpus_stats.process_document(text, i)\n",
        "        if processed_doc:\n",
        "            corpus_stats.tokenized_data.append(processed_doc)\n",
        "\n",
        "        # Progress indicator - more frequent updates for larger processing\n",
        "        if i > 0 and i % 5000 == 0:\n",
        "            print(f\"\\nProcessed {i:,} documents...\")\n",
        "            print(f\"Current stats: {corpus_stats.total_sentences:,} sentences, {corpus_stats.total_words:,} words\")\n",
        "            print(f\"Current TTR: {len(corpus_stats.vocabulary) / corpus_stats.total_words:.4f}\" if corpus_stats.total_words > 0 else \"\")\n",
        "\n",
        "            # Show memory-friendly batch saving every 25,000 documents\n",
        "            if i % 25000 == 0:\n",
        "                print(f\"Saving checkpoint at {i:,} documents...\")\n",
        "                temp_stats = corpus_stats.save_data_and_statistics(f\"hindi_corpus_checkpoint_{i}\")\n",
        "                print(f\"Checkpoint saved: {temp_stats['total_sentences']:,} sentences processed so far\")\n",
        "\n",
        "        # Process more documents for comprehensive statistics\n",
        "        # You can adjust this number or remove the condition entirely for full dataset\n",
        "        if i >= 100000:  # Process 100,000 documents (remove this line for full dataset)\n",
        "            print(f\"\\nProcessed {i:,} documents - stopping for memory management...\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nProcessing complete!\")\n",
        "    print(f\"Processed {corpus_stats.processed_documents:,} documents\")\n",
        "\n",
        "    print(\"\\nSaving tokenized data and computing statistics...\")\n",
        "    stats = corpus_stats.save_data_and_statistics()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL CORPUS STATISTICS (Assignment Task 1d)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"i.   Total number of sentences: {stats['total_sentences']:,}\")\n",
        "    print(f\"ii.  Total number of words: {stats['total_words']:,}\")\n",
        "    print(f\"iii. Total number of characters: {stats['total_characters']:,}\")\n",
        "    print(f\"iv.  Average Sentence Length: {stats['average_sentence_length']} words per sentence\")\n",
        "    print(f\"v.   Average word length: {stats['average_word_length']} characters per word\")\n",
        "    print(f\"vi.  Type/Token Ratio (TTR): {stats['type_token_ratio']}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Vocabulary size: {stats['vocabulary_size']:,} unique tokens\")\n",
        "    print(f\"Processed documents: {stats['processed_documents']:,}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(f\"📊 SCALE: Processed {stats['processed_documents']:,} documents\")\n",
        "    print(f\"📁 Check 'hindi_corpus_output' directory for:\")\n",
        "    if stats['processed_documents'] > 10000:\n",
        "        print(f\"📁 tokenized_data_chunk_*.json - Chunked tokenized documents\")\n",
        "        print(f\"📁 tokenized_data_complete.pkl - Complete binary format\")\n",
        "        print(f\"📁 vocabulary_top10000.json - Top 10,000 frequent words\")\n",
        "        print(f\"📁 vocabulary_complete.pkl - Complete vocabulary\")\n",
        "    else:\n",
        "        print(f\"📁 tokenized_data.json - All tokenized documents\")\n",
        "        print(f\"📁 tokenized_data.pkl - Fast-loading binary format\")\n",
        "        print(f\"📁 vocabulary_top10000.json - Top frequent words\")\n",
        "    print(f\"📁 corpus_statistics.json - All computed statistics\")\n",
        "    print(f\"📁 statistics_report.txt - Extended human-readable report\")\n",
        "\n",
        "    print(f\"\\n🎯 All requirements completed at scale:\")\n",
        "    print(f\"✅ a. Downloaded and extracted {stats['processed_documents']:,} Hindi documents\")\n",
        "    print(f\"✅ b. Tokenized {stats['total_sentences']:,} sentences into {stats['total_words']:,} words\")\n",
        "    print(f\"✅ c. Saved all tokenized data with memory-efficient chunking\")\n",
        "    print(f\"✅ d. Computed comprehensive corpus statistics\")\n",
        "\n",
        "    print(f\"\\n📈 CORPUS INSIGHTS:\")\n",
        "    print(f\"🔤 Vocabulary richness: {stats['vocabulary_size']:,} unique tokens\")\n",
        "    print(f\"📝 Text density: {stats['total_characters']:,} characters processed\")\n",
        "    print(f\"🎭 Language diversity: TTR = {stats['type_token_ratio']} (lower = more repetitive)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Fn2uAcBNo3z3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b16d8acf-a361-4ae3-d9c6-4e06ed31c4e4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Hindi dataset...\n",
            "\n",
            "SAMPLE PROCESSING (First 5 examples):\n",
            "----------------------------------------\n",
            "\n",
            "--- Example 1 ---\n",
            "Original Paragraph:\n",
            "लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Sentences (1 total):\n",
            "1. लोगों को बिलों संबंधी सुविधा देना ही उनका काम\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['लोगों', 'को', 'बिलों', 'संबंधी', 'सुविधा', 'देना', 'ही', 'उनका', 'काम']\n",
            "\n",
            "--- Example 2 ---\n",
            "Original Paragraph:\n",
            "इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था। हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ क...\n",
            "\n",
            "Tokenized Sentences (7 total):\n",
            "1. इनेलो 1987 में उस वक्त ऐसे ही दोराहे पर खड़ी थी, जब पूर्व उपप्रधानमंत्री देवीलाल ने अपने पुत्र ओमप्रकाश चौटाला को अपना राजनीतिक उत्तराधिकारी घोषित किया था।\n",
            "2. हालांकि तब पार्टी पर देवीलाल की मजबूत पकड़ के चलते पार्टी टूटने से बच गई थी।\n",
            "... and 5 more sentences\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['इनेलो', '1987', 'में', 'उस', 'वक्त', 'ऐसे', 'ही', 'दोराहे', 'पर', 'खड़ी', 'थी', ',', 'जब', 'पूर्व', 'उपप्रधानमंत्री']\n",
            "... and 13 more tokens\n",
            "\n",
            "--- Example 3 ---\n",
            "Original Paragraph:\n",
            "जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Sentences (1 total):\n",
            "1. जहां आई थी तबाही उस घाटी क्षेत्र में खतरा ज्यादा\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['जहां', 'आई', 'थी', 'तबाही', 'उस', 'घाटी', 'क्षेत्र', 'में', 'खतरा', 'ज्यादा']\n",
            "\n",
            "--- Example 4 ---\n",
            "Original Paragraph:\n",
            "इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई। भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भ...\n",
            "\n",
            "Tokenized Sentences (2 total):\n",
            "1. इसके बाद केंद्र की ओर से प्रदेश सरकार को पीएमजीएसवाई में 200 करोड़ रुपये की राशि उपलब्ध करा दी गई।\n",
            "2. भाजपा के मीडिया प्रभारी दिवाकर सिंह ने शनिवार को बताया कि केंद्र ने प्रदेश सरकार को 200 करोड़ रुपये भेजा है।\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['इसके', 'बाद', 'केंद्र', 'की', 'ओर', 'से', 'प्रदेश', 'सरकार', 'को', 'पीएमजीएसवाई', 'में', '200', 'करोड़', 'रुपये', 'की']\n",
            "... and 5 more tokens\n",
            "\n",
            "--- Example 5 ---\n",
            "Original Paragraph:\n",
            "यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा, \"हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था. हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा. इसमें स...\n",
            "\n",
            "Tokenized Sentences (6 total):\n",
            "1. यह पूछने पर कि इस बड़े मैच से पहले उनकी नींद गायब हुई तो बाबर ने कहा, \"हम काफी टूर्नामेंट खेल चुके हैं, हमने चैम्पियंस ट्राफी में भी अच्छा किया था.\n",
            "2. हम इसे जितना सरल रखेंगे, उतना ही बेहतर होगा.\n",
            "... and 4 more sentences\n",
            "\n",
            "Tokenized Words (first sentence):\n",
            "['यह', 'पूछने', 'पर', 'कि', 'इस', 'बड़े', 'मैच', 'से', 'पहले', 'उनकी', 'नींद', 'गायब', 'हुई', 'तो', 'बाबर']\n",
            "... and 20 more tokens\n",
            "\n",
            "Processed 5,000 documents...\n",
            "Current stats: 8,688 sentences, 149,979 words\n",
            "Current TTR: 0.1198\n",
            "\n",
            "Processed 10,000 documents...\n",
            "Current stats: 17,466 sentences, 304,856 words\n",
            "Current TTR: 0.0893\n",
            "\n",
            "Processed 15,000 documents...\n",
            "Current stats: 26,652 sentences, 466,054 words\n",
            "Current TTR: 0.0752\n",
            "\n",
            "Processed 20,000 documents...\n",
            "Current stats: 35,399 sentences, 617,723 words\n",
            "Current TTR: 0.0664\n",
            "\n",
            "Processed 25,000 documents...\n",
            "Current stats: 44,176 sentences, 771,322 words\n",
            "Current TTR: 0.0604\n",
            "Saving checkpoint at 25,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 44,176 sentences processed so far\n",
            "\n",
            "Processed 30,000 documents...\n",
            "Current stats: 53,013 sentences, 923,722 words\n",
            "Current TTR: 0.0557\n",
            "\n",
            "Processed 35,000 documents...\n",
            "Current stats: 61,795 sentences, 1,077,612 words\n",
            "Current TTR: 0.0521\n",
            "\n",
            "Processed 40,000 documents...\n",
            "Current stats: 70,596 sentences, 1,232,808 words\n",
            "Current TTR: 0.0492\n",
            "\n",
            "Processed 45,000 documents...\n",
            "Current stats: 79,111 sentences, 1,379,957 words\n",
            "Current TTR: 0.0470\n",
            "\n",
            "Processed 50,000 documents...\n",
            "Current stats: 87,557 sentences, 1,527,390 words\n",
            "Current TTR: 0.0448\n",
            "Saving checkpoint at 50,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 87,557 sentences processed so far\n",
            "\n",
            "Processed 55,000 documents...\n",
            "Current stats: 96,112 sentences, 1,676,672 words\n",
            "Current TTR: 0.0429\n",
            "\n",
            "Processed 60,000 documents...\n",
            "Current stats: 105,510 sentences, 1,844,378 words\n",
            "Current TTR: 0.0411\n",
            "\n",
            "Processed 65,000 documents...\n",
            "Current stats: 114,523 sentences, 2,007,109 words\n",
            "Current TTR: 0.0398\n",
            "\n",
            "Processed 70,000 documents...\n",
            "Current stats: 123,771 sentences, 2,171,661 words\n",
            "Current TTR: 0.0385\n",
            "\n",
            "Processed 75,000 documents...\n",
            "Current stats: 132,733 sentences, 2,326,772 words\n",
            "Current TTR: 0.0376\n",
            "Saving checkpoint at 75,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 132,733 sentences processed so far\n",
            "\n",
            "Processed 80,000 documents...\n",
            "Current stats: 141,561 sentences, 2,480,205 words\n",
            "Current TTR: 0.0366\n",
            "\n",
            "Processed 85,000 documents...\n",
            "Current stats: 149,854 sentences, 2,623,937 words\n",
            "Current TTR: 0.0357\n",
            "\n",
            "Processed 90,000 documents...\n",
            "Current stats: 158,799 sentences, 2,778,075 words\n",
            "Current TTR: 0.0348\n",
            "\n",
            "Processed 95,000 documents...\n",
            "Current stats: 167,383 sentences, 2,926,773 words\n",
            "Current TTR: 0.0340\n",
            "\n",
            "Processed 100,000 documents...\n",
            "Current stats: 175,683 sentences, 3,071,468 words\n",
            "Current TTR: 0.0333\n",
            "Saving checkpoint at 100,000 documents...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "Checkpoint saved: 175,683 sentences processed so far\n",
            "\n",
            "Processed 100,000 documents - stopping for memory management...\n",
            "\n",
            "Processing complete!\n",
            "Processed 50,001 documents\n",
            "\n",
            "Saving tokenized data and computing statistics...\n",
            "Saving large dataset in chunks of 10000 documents...\n",
            "\n",
            "============================================================\n",
            "FINAL CORPUS STATISTICS (Assignment Task 1d)\n",
            "============================================================\n",
            "i.   Total number of sentences: 175,683\n",
            "ii.  Total number of words: 3,071,468\n",
            "iii. Total number of characters: 14,526,607\n",
            "iv.  Average Sentence Length: 17.48 words per sentence\n",
            "v.   Average word length: 3.84 characters per word\n",
            "vi.  Type/Token Ratio (TTR): 0.0333\n",
            "------------------------------------------------------------\n",
            "Vocabulary size: 102,326 unique tokens\n",
            "Processed documents: 50,001\n",
            "============================================================\n",
            "📊 SCALE: Processed 50,001 documents\n",
            "📁 Check 'hindi_corpus_output' directory for:\n",
            "📁 tokenized_data_chunk_*.json - Chunked tokenized documents\n",
            "📁 tokenized_data_complete.pkl - Complete binary format\n",
            "📁 vocabulary_top10000.json - Top 10,000 frequent words\n",
            "📁 vocabulary_complete.pkl - Complete vocabulary\n",
            "📁 corpus_statistics.json - All computed statistics\n",
            "📁 statistics_report.txt - Extended human-readable report\n",
            "\n",
            "🎯 All requirements completed at scale:\n",
            "✅ a. Downloaded and extracted 50,001 Hindi documents\n",
            "✅ b. Tokenized 175,683 sentences into 3,071,468 words\n",
            "✅ c. Saved all tokenized data with memory-efficient chunking\n",
            "✅ d. Computed comprehensive corpus statistics\n",
            "\n",
            "📈 CORPUS INSIGHTS:\n",
            "🔤 Vocabulary richness: 102,326 unique tokens\n",
            "📝 Text density: 14,526,607 characters processed\n",
            "🎭 Language diversity: TTR = 0.0333 (lower = more repetitive)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xpgkRa_xbSc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}